{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safari-mohammadreza/MWL_GCN_GAT/blob/main/GCN_MWL_Paper_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V6pCR08bYmJb",
      "metadata": {
        "id": "V6pCR08bYmJb"
      },
      "source": [
        "# Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OuS-gVfkf_hh",
      "metadata": {
        "collapsed": true,
        "id": "OuS-gVfkf_hh"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "# !pip install optuna\n",
        "# !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric \\\n",
        "#     -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "# !pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a856d6cd",
      "metadata": {
        "collapsed": true,
        "id": "a856d6cd"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Numerical & data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "# PyTorch core\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch Geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "# Scikit‑learn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve,\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "# import optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "HLy-FQPXUziF"
      },
      "id": "HLy-FQPXUziF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23606b53",
      "metadata": {
        "id": "23606b53"
      },
      "outputs": [],
      "source": [
        "# variables\n",
        "\n",
        "path='/content/gdrive/My Drive/Thesis/connectivities_mat_files/'\n",
        "save_path='/content/gdrive/My Drive/GCN/save'\n",
        "save_fig='/content/gdrive/My Drive/GCN/figure'\n",
        "\n",
        "bands=['delta','theta','alpha','beta','gamma']\n",
        "channels= ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
        "category_labels = ['low', 'high']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yuM9S_XtNlKu",
      "metadata": {
        "id": "yuM9S_XtNlKu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "\n",
        "def prepare_graphs(data_dict, labels, bands, cache_path=None):\n",
        "    \"\"\"\n",
        "    Prepare and cache graph.Data objects from connectivity matrices.\n",
        "    Args:\n",
        "        data_dict (dict): {band: np.ndarray of shape (N, C, C)}\n",
        "        labels (dict): {band: np.ndarray of shape (N,)}\n",
        "        bands (list): list of band names to include\n",
        "        cache_path (str): optional path to pickle cache\n",
        "    Returns:\n",
        "        List[Data]\n",
        "    \"\"\"\n",
        "    if cache_path and os.path.exists(cache_path):\n",
        "        return pickle.load(open(cache_path, 'rb'))\n",
        "    graphs = []\n",
        "    for band in bands:\n",
        "        arr = data_dict[band]\n",
        "        lab = labels[band]\n",
        "        for i in range(arr.shape[0]):\n",
        "            adj = arr[i].copy()\n",
        "            np.fill_diagonal(adj, 0)\n",
        "            edge_index = torch.tensor(np.vstack(np.where(adj > 0)), dtype=torch.long)\n",
        "            edge_attr = torch.tensor(adj[adj > 0], dtype=torch.float)\n",
        "            x = torch.tensor(adj, dtype=torch.float32)\n",
        "            y = torch.tensor([lab[i]], dtype=torch.long)\n",
        "            graphs.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y))\n",
        "    if cache_path:\n",
        "        pickle.dump(graphs, open(cache_path, 'wb'))\n",
        "    return graphs\n",
        "\n",
        "\n",
        "def plot_connectivity_sample(data_dict, bands, channel_labels, categories):\n",
        "    \"\"\"\n",
        "    Plot a single example connectivity matrix per band and category.\n",
        "    \"\"\"\n",
        "    for band in bands:\n",
        "        for cat in categories:\n",
        "            key = f\"{band}_{cat}\"\n",
        "            if key not in data_dict:\n",
        "                continue\n",
        "            mat = data_dict[key][0].copy()\n",
        "            np.fill_diagonal(mat, 0)\n",
        "\n",
        "            plt.figure(figsize=(6,6))\n",
        "            sns.heatmap(mat, cmap='viridis', xticklabels=channel_labels,\n",
        "                        yticklabels=channel_labels, square=True)\n",
        "            plt.title(f\"{band.capitalize()} Connectivity ({cat})\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
        "\n",
        "class AblationGCN_GAT(nn.Module):\n",
        "    \"\"\"\n",
        "    GNN model with optional GAT, residual, and dual pooling ablations.\n",
        "    Now accepts `edge_attr` (mapped to edge_weight) and ignores extra kwargs.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, hid_ch, out_ch,\n",
        "                 use_gat=True, use_res=True, use_max=True,\n",
        "                 fc1=128, dropout=0.3, heads=4):\n",
        "        super().__init__()\n",
        "        self.use_gat, self.use_res, self.use_max = use_gat, use_res, use_max\n",
        "\n",
        "        self.gcn1 = GCNConv(in_ch, hid_ch)\n",
        "        self.bn1  = nn.BatchNorm1d(hid_ch)\n",
        "\n",
        "        if use_gat:\n",
        "            self.gat1 = GATConv(hid_ch, hid_ch, heads=heads, concat=False)\n",
        "            self.bn2  = nn.BatchNorm1d(hid_ch)\n",
        "\n",
        "        self.gcn2 = GCNConv(hid_ch, hid_ch)\n",
        "\n",
        "        pooled_dim = hid_ch * (1 + int(use_max))\n",
        "        self.fc1  = nn.Linear(pooled_dim, fc1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc2  = nn.Linear(fc1, out_ch)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None,\n",
        "                edge_weight=None, edge_attr=None, **kwargs):\n",
        "        # If the explainer passed `edge_attr`, treat it as edge_weight\n",
        "        if edge_attr is not None and edge_weight is None:\n",
        "            edge_weight = edge_attr\n",
        "\n",
        "        # First GCN layer\n",
        "        x = self.gcn1(x, edge_index, edge_weight)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # Optional GAT layer\n",
        "        if self.use_gat:\n",
        "            x = self.gat1(x, edge_index)\n",
        "            x = F.relu(self.bn2(x))\n",
        "            x = self.drop(x)\n",
        "\n",
        "        # Residual GCN\n",
        "        res = self.gcn2(x, edge_index, edge_weight)\n",
        "        x   = x + res if self.use_res else res\n",
        "\n",
        "        # Global pooling (need `batch` for batching multiple graphs)\n",
        "        if batch is None:\n",
        "            # assume a single graph: all nodes belong to batch 0\n",
        "            batch = x.new_zeros(x.size(0), dtype=torch.long)\n",
        "\n",
        "        m = global_mean_pool(x, batch)\n",
        "        if self.use_max:\n",
        "            M = global_max_pool(x, batch)\n",
        "            m = torch.cat([m, M], dim=1)\n",
        "\n",
        "        # MLP head\n",
        "        x = F.relu(self.fc1(m))\n",
        "        x = self.drop(x)\n",
        "        out = self.fc2(x)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "def objective(trial, graphs):\n",
        "    \"\"\"\n",
        "    Optuna objective for hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    in_ch = graphs[0].x.size(1)\n",
        "    hidden = trial.suggest_categorical('hidden', [64,128,256])\n",
        "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
        "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
        "    n_splits = trial.suggest_int('folds', 8, 11)\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    accs = []\n",
        "    for train_idx, val_idx in kf.split(graphs):\n",
        "        train = [graphs[i] for i in train_idx]\n",
        "        val   = [graphs[i] for i in val_idx]\n",
        "        tl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "        vl = DataLoader(val, batch_size=32)\n",
        "\n",
        "        model = AblationGCN_GAT(in_ch, hidden, 2,\n",
        "                                 use_gat=True, use_res=True, use_max=True,\n",
        "                                 fc1=hidden, dropout=dropout).to('cuda')\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        best=0; patience=0\n",
        "        for epoch in range(50):\n",
        "            model.train()\n",
        "            for b in tl:\n",
        "                b=b.to('cuda'); opt.zero_grad()\n",
        "                out = model(b.x,b.edge_index,b.batch)\n",
        "                loss_fn(out,b.y).backward(); opt.step()\n",
        "            model.eval()\n",
        "            preds, trues = [], []\n",
        "            with torch.no_grad():\n",
        "                for b in vl:\n",
        "                    b=b.to('cuda')\n",
        "                    o=model(b.x,b.edge_index,b.batch)\n",
        "                    preds+=o.argmax(1).cpu().tolist()\n",
        "                    trues+=b.y.cpu().tolist()\n",
        "            acc = accuracy_score(trues,preds)\n",
        "            if acc>best: best,patience=acc,0\n",
        "            else: patience+=1\n",
        "            if patience>5: break\n",
        "        accs.append(best)\n",
        "    return np.mean(accs)\n",
        "\n",
        "class GCN_GAT_Model(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5, heads=4):\n",
        "        super(GCN_GAT_Model, self).__init__()\n",
        "        self.gcn1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.dropout = dropout\n",
        "        self.gat1 = GATConv(hidden_channels, hidden_channels, heads=heads, concat=False)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.fc1 = nn.Linear(hidden_channels, 64)\n",
        "        self.fc2 = nn.Linear(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
        "        # 1) First GCN layer\n",
        "        x = self.gcn1(x, edge_index, edge_weight=edge_weight)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # 2) GAT layer\n",
        "        x = self.gat1(x, edge_index)  # GATConv doesn't support edge_weight by default\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # 3) Second GCN layer\n",
        "        x = self.gcn2(x, edge_index)\n",
        "        # 4) Pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        # 5) Final MLP layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "#####################################################\n",
        "# Enhanced Model Definition with Residual & Dual Pooling\n",
        "#####################################################\n",
        "class EnhancedGCN_GAT_Model(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, fc1_size, dropout=0.3, heads=4):\n",
        "        super(EnhancedGCN_GAT_Model, self).__init__()\n",
        "        # First GCN layer\n",
        "        self.gcn1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.dropout = dropout\n",
        "        # GAT layer\n",
        "        self.gat1 = GATConv(hidden_channels, hidden_channels, heads=heads, concat=False)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        # Second GCN layer\n",
        "        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        # Final FC layers after dual pooling (concatenating mean and max pool)\n",
        "        self.fc1 = nn.Linear(2 * hidden_channels, fc1_size)\n",
        "        self.fc2 = nn.Linear(fc1_size, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
        "        # 1) First GCN layer\n",
        "        x = self.gcn1(x, edge_index, edge_weight=edge_weight)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # 2) GAT layer\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # 3) Second GCN layer with Residual Connection\n",
        "        x2 = self.gcn2(x, edge_index)\n",
        "        x = x + x2  # Residual addition\n",
        "        # 4) Dual Pooling: mean and max pooling\n",
        "        mean_pool = global_mean_pool(x, batch)\n",
        "        max_pool = global_max_pool(x, batch)\n",
        "        x = torch.cat([mean_pool, max_pool], dim=1)\n",
        "        # 5) Final FC layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "EGpzyma1Y3Wb"
      },
      "id": "EGpzyma1Y3Wb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8DJbSOFFkr1u",
      "metadata": {
        "collapsed": true,
        "id": "8DJbSOFFkr1u"
      },
      "outputs": [],
      "source": [
        "# load connectivity matrices\n",
        "with open(path+'hi_delta', 'rb') as f:\n",
        "    hi_delta = pickle.load(f)\n",
        "print(np.shape(hi_delta)) # (1776, 14, 14)\n",
        "with open(path+'lo_delta', 'rb') as f:\n",
        "    lo_delta = pickle.load(f)\n",
        "print(np.shape(lo_delta))\n",
        "\n",
        "with open(path+'hi_theta', 'rb') as f:\n",
        "    hi_theta = pickle.load(f)\n",
        "print(np.shape(hi_theta))\n",
        "with open(path+'lo_theta', 'rb') as f:\n",
        "    lo_theta = pickle.load(f)\n",
        "print(np.shape(lo_theta))\n",
        "\n",
        "with open(path+'hi_alpha', 'rb') as f:\n",
        "    hi_alpha = pickle.load(f)\n",
        "print(np.shape(hi_alpha))\n",
        "with open(path+'lo_alpha', 'rb') as f:\n",
        "    lo_alpha = pickle.load(f)\n",
        "print(np.shape(lo_alpha))\n",
        "\n",
        "with open(path+'hi_beta', 'rb') as f:\n",
        "    hi_beta = pickle.load(f)\n",
        "print(np.shape(hi_beta))\n",
        "with open(path+'lo_beta', 'rb') as f:\n",
        "    lo_beta = pickle.load(f)\n",
        "print(np.shape(lo_beta))\n",
        "\n",
        "with open(path+'hi_gamma', 'rb') as f:\n",
        "    hi_gamma = pickle.load(f)\n",
        "print(np.shape(hi_gamma))\n",
        "with open(path+'lo_gamma', 'rb') as f:\n",
        "    lo_gamma = pickle.load(f)\n",
        "print(np.shape(lo_gamma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VIb6IOAPyLtB",
      "metadata": {
        "id": "VIb6IOAPyLtB"
      },
      "outputs": [],
      "source": [
        "data = [hi_delta, lo_delta, hi_theta, lo_theta, hi_alpha, lo_alpha, hi_beta, lo_beta, hi_gamma, lo_gamma]\n",
        "print(np.shape(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4GYKcJbX8qmY",
      "metadata": {
        "id": "4GYKcJbX8qmY"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E3igkLx-EKud",
      "metadata": {
        "id": "E3igkLx-EKud",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "# Load Your Data\n",
        "#####################################################\n",
        "# hi_delta, lo_delta, hi_theta, etc. should be preloaded numpy arrays.\n",
        "data_dict = {\n",
        "    \"delta\": np.concatenate((hi_delta, lo_delta)),\n",
        "    \"theta\": np.concatenate((hi_theta, lo_theta)),\n",
        "    \"alpha\": np.concatenate((hi_alpha, lo_alpha)),\n",
        "    \"beta\": np.concatenate((hi_beta, lo_beta)),\n",
        "    \"gamma\": np.concatenate((hi_gamma, lo_gamma)),\n",
        "}\n",
        "\n",
        "labels = {}\n",
        "for band in ['delta', 'theta', 'alpha', 'beta', 'gamma']:\n",
        "    num_total = data_dict[band].shape[0]\n",
        "    hi = data_dict[band][:num_total // 2]\n",
        "    lo = data_dict[band][num_total // 2:]\n",
        "    labels[band] = np.concatenate((np.ones(hi.shape[0]), np.zeros(lo.shape[0]))).astype(int)\n",
        "    print(f\"Label distribution for {band}:\", np.bincount(labels[band]))\n",
        "\n",
        "bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
        "graphs = prepare_graphs(data_dict, labels, bands)\n",
        "\n",
        "#####################################################\n",
        "# Run Hyperparameter Optimization with Optuna\n",
        "#####################################################\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=15)  # Adjust number of trials as needed\n",
        "\n",
        "# Print the best trial and its hyperparameters:\n",
        "print(\"Best trial:\")\n",
        "best_trial = study.best_trial\n",
        "print(f\"  Value: {best_trial.value}\")\n",
        "print(\"  Params:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "#####################################################\n",
        "# Optionally, Plot the Optimization History\n",
        "#####################################################\n",
        "optuna.visualization.plot_optimization_history(study)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5KAnOFz6oaZ-",
      "metadata": {
        "id": "5KAnOFz6oaZ-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "# Load Your Data\n",
        "#####################################################\n",
        "# Each shape: (1776, 14, 14)\n",
        "# hi_delta, lo_delta, hi_theta, etc. should be preloaded numpy arrays.\n",
        "data_dict = {\n",
        "    \"delta\": np.concatenate((hi_delta, lo_delta)),\n",
        "    \"theta\": np.concatenate((hi_theta, lo_theta)),\n",
        "    \"alpha\": np.concatenate((hi_alpha, lo_alpha)),\n",
        "    \"beta\": np.concatenate((hi_beta, lo_beta)),\n",
        "    \"gamma\": np.concatenate((hi_gamma, lo_gamma)),\n",
        "}\n",
        "\n",
        "labels = {}\n",
        "for band in ['delta', 'theta', 'alpha', 'beta', 'gamma']:\n",
        "    num_total = data_dict[band].shape[0]\n",
        "    hi = data_dict[band][:num_total // 2]\n",
        "    lo = data_dict[band][num_total // 2:]\n",
        "    labels[band] = np.concatenate((np.ones(hi.shape[0]), np.zeros(lo.shape[0]))).astype(int)\n",
        "    print(f\"Label distribution for {band}:\", np.bincount(labels[band]))\n",
        "\n",
        "bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
        "graphs = prepare_graphs(data_dict, labels, bands)\n",
        "\n",
        "#####################################################\n",
        "# Run Hyperparameter Optimization with Optuna\n",
        "#####################################################\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=15)  # Adjust number of trials as needed\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "#####################################################\n",
        "# Optionally, Plot the Optimization History\n",
        "#####################################################\n",
        "optuna.visualization.plot_optimization_history(study)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JtFeaLdfZXw1",
      "metadata": {
        "id": "JtFeaLdfZXw1"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w0Hd_tqnkNpk",
      "metadata": {
        "collapsed": true,
        "id": "w0Hd_tqnkNpk"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "# Load Data\n",
        "#####################################################\n",
        "# hi_delta, lo_delta, hi_theta, lo_theta, hi_alpha, lo_alpha, hi_beta, lo_beta, hi_gamma, lo_gamma\n",
        "# should be preloaded numpy arrays of shape (1776, 14, 14) each.\n",
        "data_dict = {\n",
        "    \"delta\": np.concatenate((hi_delta, lo_delta)),\n",
        "    \"theta\": np.concatenate((hi_theta, lo_theta)),\n",
        "    \"alpha\": np.concatenate((hi_alpha, lo_alpha)),\n",
        "    \"beta\": np.concatenate((hi_beta, lo_beta)),\n",
        "    \"gamma\": np.concatenate((hi_gamma, lo_gamma)),\n",
        "}\n",
        "\n",
        "labels = {}\n",
        "for band in ['delta', 'theta', 'alpha', 'beta', 'gamma']:\n",
        "    num_total = data_dict[band].shape[0]\n",
        "    hi = data_dict[band][:num_total // 2]\n",
        "    lo = data_dict[band][num_total // 2:]\n",
        "    labels[band] = np.concatenate((np.ones(hi.shape[0]), np.zeros(lo.shape[0]))).astype(int)\n",
        "    print(f\"Label distribution for {band}:\", np.bincount(labels[band]))\n",
        "\n",
        "bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
        "graphs = prepare_graphs(data_dict, labels, bands)\n",
        "\n",
        "#####################################################\n",
        "# Cross-Validation and Training\n",
        "#####################################################\n",
        "\n",
        "# Use 11-fold cross-validation\n",
        "kf = KFold(n_splits=13, shuffle=True, random_state=42)\n",
        "num_epochs = 120  # Increased epochs\n",
        "early_stopping_patience = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_acc_history_all = []\n",
        "val_acc_history_all = []\n",
        "fold_preds_all = []\n",
        "fold_trues_all = []\n",
        "\n",
        "# Using tuned hyperparameters\n",
        "hidden_channels = 512\n",
        "fc1_size = 128\n",
        "dropout = 0.3\n",
        "learning_rate = 0.0007\n",
        "weight_decay = 1e-4  # added weight decay\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(graphs)):\n",
        "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
        "    train_set = [graphs[i] for i in train_idx]\n",
        "    test_set = [graphs[i] for i in test_idx]\n",
        "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=32)\n",
        "\n",
        "    model = GCN_GAT_Model(\n",
        "        in_channels=14,\n",
        "        hidden_channels=hidden_channels,\n",
        "        out_channels=2,\n",
        "        fc1_size=fc1_size,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    # Use a scheduler to reduce LR when validation plateaus\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "    # For balanced data\n",
        "    class_weights = torch.tensor([1.0, 1.0]).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience = 0\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_preds, train_trues = [], []\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            # Optionally: toggle augmentation here if desired; currently using original batch.\n",
        "            batch_used = batch\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch_used.x, batch_used.edge_index, batch_used.batch, edge_weight=batch_used.edge_attr)\n",
        "            loss = loss_fn(out, batch_used.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            train_trues.extend(batch_used.y.cpu().numpy())\n",
        "        train_acc = accuracy_score(train_trues, train_preds)\n",
        "\n",
        "        model.eval()\n",
        "        val_preds, val_trues = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch, edge_weight=batch.edge_attr)\n",
        "                val_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "                val_trues.extend(batch.y.cpu().numpy())\n",
        "        val_acc = accuracy_score(val_trues, val_preds)\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        train_acc_history.append(train_acc)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict()\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        if patience > early_stopping_patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    final_val_preds, final_val_trues = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch, edge_weight=batch.edge_attr)\n",
        "            final_val_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            final_val_trues.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    train_acc_history_all.append(train_acc_history)\n",
        "    val_acc_history_all.append(val_acc_history)\n",
        "    fold_preds_all.append(final_val_preds)\n",
        "    fold_trues_all.append(final_val_trues)\n",
        "\n",
        "#####################################################\n",
        "# Plot Per-Fold Accuracy Curves and Confusion Matrices\n",
        "#####################################################\n",
        "fig, axs = plt.subplots(nrows=len(train_acc_history_all), ncols=2, figsize=(12, 4 * len(train_acc_history_all)))\n",
        "\n",
        "for i, (train_accs, val_accs) in enumerate(zip(train_acc_history_all, val_acc_history_all)):\n",
        "    epochs = range(1, len(train_accs) + 1)\n",
        "    axs[i, 0].plot(epochs, train_accs, label='Train Acc')\n",
        "    axs[i, 0].plot(epochs, val_accs, label='Val Acc')\n",
        "    axs[i, 0].set_xlabel('Epoch')\n",
        "    axs[i, 0].set_ylabel('Accuracy')\n",
        "    axs[i, 0].set_title(f'Fold {i+1} Accuracy')\n",
        "    axs[i, 0].legend()\n",
        "    cm = confusion_matrix(fold_trues_all[i], fold_preds_all[i])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Low', 'High'],\n",
        "                yticklabels=['Low', 'High'],\n",
        "                ax=axs[i, 1])\n",
        "    axs[i, 1].set_title(f'Fold {i+1} Confusion Matrix')\n",
        "    axs[i, 1].set_xlabel('Predicted')\n",
        "    axs[i, 1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "# Overall Results Across Folds\n",
        "#####################################################\n",
        "all_preds = np.concatenate(fold_preds_all, axis=0)\n",
        "all_labels = np.concatenate(fold_trues_all, axis=0)\n",
        "\n",
        "accuracy_final = accuracy_score(all_labels, all_preds)\n",
        "precision_final = precision_score(all_labels, all_preds)\n",
        "recall_final = recall_score(all_labels, all_preds)\n",
        "f1_final = f1_score(all_labels, all_preds)\n",
        "auc_final = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "print(f\"\\n=== Final Metrics Across All Folds ===\")\n",
        "print(f\"Accuracy: {accuracy_final:.4f}\")\n",
        "print(f\"Precision: {precision_final:.4f}\")\n",
        "print(f\"Recall: {recall_final:.4f}\")\n",
        "print(f\"F1 Score: {f1_final:.4f}\")\n",
        "print(f\"AUC Score: {auc_final:.4f}\")\n",
        "\n",
        "precisions, recalls, _ = precision_recall_curve(all_labels, all_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(recalls, precisions, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Overall Precision-Recall Curve\")\n",
        "plt.show()\n",
        "\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Overall Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "# Plot Average Training/Validation Accuracy\n",
        "#####################################################\n",
        "max_epochs = max(len(acc) for acc in train_acc_history_all)\n",
        "train_acc_padded = np.array([np.pad(acc, (0, max_epochs - len(acc)), 'edge')\n",
        "                             for acc in train_acc_history_all])\n",
        "val_acc_padded = np.array([np.pad(acc, (0, max_epochs - len(acc)), 'edge')\n",
        "                           for acc in val_acc_history_all])\n",
        "train_acc_mean = np.mean(train_acc_padded, axis=0)\n",
        "val_acc_mean = np.mean(val_acc_padded, axis=0)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_acc_mean, label='Training Accuracy')\n",
        "plt.plot(val_acc_mean, label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Average Training and Validation Accuracy Curve Across Folds\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a_yztlBpalAO",
      "metadata": {
        "collapsed": true,
        "id": "a_yztlBpalAO"
      },
      "outputs": [],
      "source": [
        "# Ablation purpose and per band\n",
        "\n",
        "# Assume hi_delta, lo_delta, etc. are loaded numpy arrays of shape (1776,14,14)\n",
        "bands = ['delta','theta','alpha','beta','gamma']\n",
        "data_dict = {\n",
        "    b: np.concatenate((globals()[f\"hi_{b}\"], globals()[f\"lo_{b}\"]))\n",
        "    for b in bands\n",
        "}\n",
        "labels = {}\n",
        "for b in bands:\n",
        "    arr = data_dict[b]\n",
        "    halfway = arr.shape[0] // 2\n",
        "    labels[b] = np.concatenate((np.ones(halfway), np.zeros(halfway))).astype(int)\n",
        "\n",
        "# Create dirs\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 3) Main training loop with ablations + history\n",
        "# ------------------------------------------------------\n",
        "scenarios = bands + ['all']\n",
        "ablations = [\n",
        "    dict(name='full',      use_gat=True,  use_res=True,  use_max=True),\n",
        "    dict(name='no_gat',    use_gat=False, use_res=True,  use_max=True),\n",
        "    dict(name='no_res',    use_gat=True,  use_res=False, use_max=True),\n",
        "    dict(name='mean_only', use_gat=True,  use_res=True,  use_max=False),\n",
        "]\n",
        "\n",
        "history = {}   # key -> {'train': [list per fold], 'val': [list per fold]}\n",
        "results = {}   # key -> mean accuracy\n",
        "\n",
        "for scenario in scenarios:\n",
        "    cache = f'checkpoints/graphs_{scenario}.pkl'\n",
        "    if scenario=='all':\n",
        "        gs = prepare_graphs(data_dict, labels, bands, cache)\n",
        "    else:\n",
        "        gs = prepare_graphs({scenario:data_dict[scenario]},\n",
        "                            {scenario:labels[scenario]},\n",
        "                            [scenario], cache)\n",
        "\n",
        "    for abl in ablations:\n",
        "        key = f\"{scenario}/{abl['name']}\"\n",
        "        history[key] = {'train':[], 'val':[]}\n",
        "\n",
        "        ckpt_w = f'checkpoints/{scenario}_{abl[\"name\"]}.pt'\n",
        "        ckpt_a = ckpt_w + '_acc'\n",
        "        if os.path.exists(ckpt_w) and os.path.exists(ckpt_a):\n",
        "            results[key] = torch.load(ckpt_a)\n",
        "            print(f\"Skipping {key}, loaded acc {results[key]:.4f}\")\n",
        "            continue\n",
        "\n",
        "        fold_acc = []\n",
        "        kf = KFold(n_splits=11, shuffle=True, random_state=42)\n",
        "\n",
        "        for fold, (ti,vi) in enumerate(kf.split(gs),1):\n",
        "            train_set = [gs[i] for i in ti]\n",
        "            test_set  = [gs[i] for i in vi]\n",
        "            train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "            test_loader  = DataLoader(test_set,  batch_size=32)\n",
        "\n",
        "            model = AblationGCN_GAT(14,256,2,128,0.274,4,\n",
        "                                    use_gat=abl['use_gat'],\n",
        "                                    use_res=abl['use_res'],\n",
        "                                    use_max=abl['use_max']).to(device)\n",
        "            opt     = torch.optim.Adam(model.parameters(), lr=7e-4, weight_decay=1e-4)\n",
        "            sched   = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'max', patience=3)\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "            best,pat = 0,0\n",
        "            train_hist, val_hist = [], []\n",
        "\n",
        "            for ep in range(1,121):\n",
        "                # --- train ---\n",
        "                model.train()\n",
        "                train_preds, train_trues = [], []\n",
        "                for b in train_loader:\n",
        "                    b = b.to(device)\n",
        "                    opt.zero_grad()\n",
        "                    out = model(b.x, b.edge_index, b.batch)\n",
        "                    loss = loss_fn(out, b.y)\n",
        "                    loss.backward(); opt.step()\n",
        "                    train_preds.extend(out.argmax(1).cpu().numpy())\n",
        "                    train_trues.extend(b.y.cpu().numpy())\n",
        "                train_acc = accuracy_score(train_trues, train_preds)\n",
        "\n",
        "                # --- val ---\n",
        "                model.eval()\n",
        "                val_preds, val_trues = [], []\n",
        "                with torch.no_grad():\n",
        "                    for b in test_loader:\n",
        "                        b = b.to(device)\n",
        "                        out = model(b.x, b.edge_index, b.batch)\n",
        "                        val_preds.extend(out.argmax(1).cpu().numpy())\n",
        "                        val_trues.extend(b.y.cpu().numpy())\n",
        "                val_acc = accuracy_score(val_trues, val_preds)\n",
        "                sched.step(val_acc)\n",
        "\n",
        "                # record histories\n",
        "                train_hist.append(train_acc)\n",
        "                val_hist.append(val_acc)\n",
        "\n",
        "                # early‑stop & checkpoint\n",
        "                if val_acc > best:\n",
        "                    best, pat = val_acc, 0\n",
        "                    torch.save(model.state_dict(), ckpt_w)\n",
        "                else:\n",
        "                    pat += 1\n",
        "                if pat > 10:\n",
        "                    break\n",
        "\n",
        "            history[key]['train'].append(train_hist)\n",
        "            history[key]['val'].append(val_hist)\n",
        "            fold_acc.append(best)\n",
        "            print(f\"{key} fold{fold} best-val-acc={best:.4f}\")\n",
        "\n",
        "        mean_acc = float(np.mean(fold_acc))\n",
        "        results[key] = mean_acc\n",
        "        torch.save(mean_acc, ckpt_a)\n",
        "        print(f\"=> {key} mean acc={mean_acc:.4f}\")\n",
        "\n",
        "# 4) Save history for later plotting\n",
        "with open('checkpoints/history.pkl','wb') as f:\n",
        "    pickle.dump(history, f)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 5) Plot bar chart of overall scenario accuracies\n",
        "# ----------------------------------------------------------------\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(results, orient='index', columns=['Mean Accuracy'])\n",
        "df.index.name = 'Scenario'\n",
        "df = df.reset_index().sort_values('Mean Accuracy', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=df, x='Scenario', y='Mean Accuracy', palette='magma')\n",
        "plt.xticks(rotation=45, ha='right'); plt.ylim(0,1)\n",
        "plt.title('Validation Accuracy per Scenario & Ablation')\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/accuracy_bar.png')\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 6) Plot train/val accuracy curves for each key\n",
        "# ----------------------------------------------------------------\n",
        "with open('checkpoints/history.pkl','rb') as f:\n",
        "    history = pickle.load(f)\n",
        "\n",
        "for key, h in history.items():\n",
        "    if not h['val'] or not h['train']:\n",
        "        continue\n",
        "\n",
        "    max_ep = max(max(len(l) for l in h['train']),\n",
        "                 max(len(l) for l in h['val']))\n",
        "    train_arr = np.array([np.pad(l, (0, max_ep-len(l)), 'edge') for l in h['train']])\n",
        "    val_arr   = np.array([np.pad(l, (0, max_ep-len(l)), 'edge') for l in h['val']])\n",
        "\n",
        "    tm = train_arr.mean(axis=0)\n",
        "    vm = val_arr.mean(axis=0)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(range(1, len(tm)+1), tm, label='Train Acc')\n",
        "    plt.plot(range(1, len(vm)+1), vm, label='Val Acc')\n",
        "    plt.title(f'Train vs Val — {key}')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(f'plots/{key.replace(\"/\",\"_\")}_curve.png')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretability"
      ],
      "metadata": {
        "id": "cOEETucdax_8"
      },
      "id": "cOEETucdax_8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C74lDUIWOoub",
      "metadata": {
        "id": "C74lDUIWOoub"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.explain.algorithm import GNNExplainer\n",
        "from torch_geometric.explain import Explainer\n",
        "\n",
        "# 1) Paths on your Drive\n",
        "save_path    = '/content/gdrive/My Drive/GCN/save/checkpoints'\n",
        "model_path   = os.path.join(save_path, 'all_full.pt')\n",
        "graph_path   = os.path.join(save_path, 'graphs_all.pkl')\n",
        "\n",
        "# 2) Validate existence\n",
        "assert os.path.isfile(model_path), f\"Missing model file: {model_path}\"\n",
        "assert os.path.isfile(graph_path), f\"Missing graph cache: {graph_path}\"\n",
        "print(\"✅ Found both checkpoint files.\")\n",
        "\n",
        "# 3) Load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#                        in  hid out   gat    res   max  fc1   drop heads\n",
        "model = AblationGCN_GAT( 14, 256,  2,  True, True, True, 128, 0.274,  4 ).to(device)\n",
        "state = torch.load(model_path, map_location=device)\n",
        "print(\"State dict keys (first 5):\", list(state.keys())[:5], \"…\")\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "# 4) Load graph cache\n",
        "with open(graph_path, 'rb') as f:\n",
        "    graphs = pickle.load(f)\n",
        "assert graphs, \"Graph cache is empty!\"\n",
        "data = graphs[0].to(device)\n",
        "print(f\"Loaded graph: x{tuple(data.x.shape)}, edges={data.edge_index.size(1)}\")\n",
        "\n",
        "# 5) Build the high‑level Explainer\n",
        "explainer = Explainer(\n",
        "    model            = model,\n",
        "    algorithm        = GNNExplainer(epochs=200),\n",
        "    explanation_type = 'model',\n",
        "    node_mask_type   = 'object',\n",
        "    edge_mask_type   = 'object',\n",
        "    model_config     = dict(\n",
        "        mode        = 'multiclass_classification',\n",
        "        task_level  = 'graph',\n",
        "        return_type = 'log_probs',            # ← corrected\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 6) Generate an Explanation\n",
        "explanation = explainer(\n",
        "    x          = data.x,\n",
        "    edge_index = data.edge_index,\n",
        "    edge_attr  = data.edge_attr,\n",
        "    batch      = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
        ")\n",
        "\n",
        "# 7) Grab the learned masks\n",
        "edge_mask = explanation.edge_mask   # shape [num_edges]\n",
        "node_mask = explanation.node_mask   # shape [num_nodes] (or None, if you chose only features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# pull out the raw edge list (as before)\n",
        "edges = data.edge_index.t().cpu().numpy()    # shape [num_edges, 2]\n",
        "\n",
        "# build a DataFrame mapping raw indices → importance\n",
        "df = pd.DataFrame({\n",
        "    'src_idx':    edges[:, 0],\n",
        "    'tgt_idx':    edges[:, 1],\n",
        "    'importance': weights                       # your normalized importance array\n",
        "})\n",
        "\n",
        "# # build a DataFrame mapping indices → names\n",
        "# df = pd.DataFrame({\n",
        "#     'source':     [channels[i] for i in edges[:, 0]],\n",
        "#     'target':     [channels[i] for i in edges[:, 1]],\n",
        "#     'importance': weights                       # your normalized importance array\n",
        "# })\n",
        "\n",
        "# sort by importance descending\n",
        "df = df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# OPTION A: keep only the very top-30 edges\n",
        "df_filtered = df.head(30)\n",
        "\n",
        "# OPTION B: keep only edges above a threshold (e.g. importance ≥ 0.8)\n",
        "# df_filtered = df[df['importance'] >= 0.8].reset_index(drop=True)\n",
        "\n",
        "# display the filtered DataFrame\n",
        "print(df_filtered.to_markdown(index=False))\n",
        "\n",
        "# --- build the 14×14 matrix ---\n",
        "num_nodes = 14\n",
        "mat = np.zeros((num_nodes, num_nodes), dtype=float)\n",
        "\n",
        "for _, row in df_filtered.iterrows():\n",
        "    i, j, w = int(row.src_idx), int(row.tgt_idx), row.importance\n",
        "    mat[i, j] = w\n",
        "    mat[j, i] = w   # comment out if your graph is directed\n",
        "\n",
        "# --- write it out to your Google Drive ---\n",
        "out_path = '/content/gdrive/My Drive/GCN/save/important_edges_top30.edge' # create .edge file and create figure with BrainNet tool in Matlab\n",
        "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "with open(out_path, 'w') as f:\n",
        "    for row in mat:\n",
        "        line = ' '.join(f\"{val:.6f}\" for val in row)\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "print(f\"Wrote 14×14 importance matrix to {out_path}\")\n"
      ],
      "metadata": {
        "id": "ejifAhNO4QTm"
      },
      "id": "ejifAhNO4QTm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JtFeaLdfZXw1"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}